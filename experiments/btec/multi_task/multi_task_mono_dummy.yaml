
dropout_rate: 0.5
cell_size: 256
embedding_size: 256
layers: 2
bidir: True
use_lstm: True
steps_per_checkpoint: 1000
steps_per_eval: 2000
max_steps: 50000

data_dir: experiments/btec/multi_task/data
model_dir: experiments/btec/multi_task/multi_task_mono_dummy
log_file: experiments/btec/multi_task/multi_task_mono_dummy/log.txt
num_samples: 0

optimizer: 'adam'
learning_rate: 0.001
initial_state_attention: True

decoder:
    name: en
    vocab_size: 30000

tasks:
  - name: task1
    train_prefix: btec.train
    dev_prefix: btec.dev
    ratio: 0.5
    encoders:
      - name: fr
        vocab_size: 30000

  - name: task2
    train_prefix: news+europarl
    dev_prefix: dev.2000
    ratio: 0.5
    encoders:
      - name: dummy
        vocab_size: 30000

main_task: task1
