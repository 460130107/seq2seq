
dropout_rate: 0.0
cell_size: 1000
embedding_size: 620
layers: 1
bidir: True
use_lstm: False
steps_per_checkpoint: 1000
steps_per_eval: 4000
max_steps: 0
optimizer: 'adadelta'
learning_rate: 0.001
batch_size: 80
num_samples: 0

data_dir: experiments/WMT14/data
model_dir: experiments/WMT14/baseline
log_file: experiments/WMT14/baseline/log.txt

max_dev_size: 2000

encoders:
  - name: en
    vocab_size: 30000

decoder:
    name: fr
    vocab_size: 30000
