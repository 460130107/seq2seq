
dropout_rate: 0.0
cell_size: 1000
attn_size: 1000
embedding_size: 620
layers: 1
bidir: True
use_lstm: False
eval_burn_in: 40000
steps_per_checkpoint: 1000
steps_per_eval: 4000
max_steps: 0
optimizer: 'adam'
weight_scale: 0.01
batch_size: 80
shuffle_data: True    # shuffle dataset at each new epoch
read_ahead: 20
max_output_len: 49    # faster decoding
max_input_len: 49     # limit memory usage
max_gradient_norm: 1.0

data_dir: experiments/WMT14/data
model_dir: experiments/WMT14/baseline
log_file: experiments/WMT14/baseline/log.txt

encoders:
  - name: en

decoder:
    name: fr

# SGD parameters
sgd_after_n_epoch: 1    # only use Adam for the first epoch, to speed up training (one epoch ~= 150k steps)
learning_rate: 0.5
learning_rate_decay_factor: 0.5
decay_every_n_epoch: 1
decay_after_n_epoch: 2
decay_if_no_progress: null
