# Same parameters as: https://github.com/facebookresearch/MIXER
# FIXME:
# - different attention model
# - different batch iteration method

dropout_rate: 0.0
cell_size: 256
embedding_size: 256
layers: 1
bidir: False
use_lstm: True
steps_per_checkpoint: 1000
steps_per_eval: 4000
max_steps: 0
optimizer: 'sgd'
learning_rate: 0.2
num_samples: 0
max_gradient_norm: 10    # TODO: check this
batch_size: 32
max_output_len: 25

data_dir: experiments/IWSLT14/data
model_dir: experiments/IWSLT14/baseline
log_file: experiments/IWSLT14/baseline/log.txt
max_dev_size: 2000
vocab_size: 0   # read vocab size from vocabulary files

encoders:
  - name: de

decoder:
    name: en
