# Multi-task training with large EN->EN

learning_rate_decay_factor: 0.95
dropout_rate: 0.5
cell_size: 256
embedding_size: 256
layers: 2
bidir: True
use_lstm: True

model_dir: experiments/pre-training/model_1
log_file: experiments/pre-training/model_1/log.txt


tasks:
  - name: task1
    data_dir: experiments/pre-training/data
    train_prefix: btec
    encoders:
      - name: fr
        vocab_size: 33598
        buckets: [ 5, 10, 20, 50]
    decoder:
        name: en
        vocab_size: 32313
        buckets: [10, 15, 25, 50]

  - name: task2
    data_dir: experiments/pre-training/data
    train_prefix: news
    encoders:
      - name: en
        vocab_size: 32313
        buckets: [10, 15, 25, 50]
    decoder:
        name: en
        vocab_size: 32313
        buckets: [10, 15, 25, 50]
