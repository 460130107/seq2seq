# BTEC (fr -> en) baseline

learning_rate_decay_factor: 0.95
dropout_rate: 0.5
cell_size: 256
embedding_size: 256
layers: 2
bidir: True
use_lstm: True
steps_per_checkpoint: 500
steps_per_eval: 2000
max_steps: 50000
max_output_len: 50

data_dir: data/btec
model_dir: models/baseline
log_file: models/baseline/log.txt

optimizer: 'sgd'
# attention: False
# optimizer: 'adam'
# learning_rate: 0.001
# buckets: [[5, 10], [10, 15], [20, 25], [50, 50]]
attention_filters: 1     # number of convolution filters to use in the attention mechanism
attention_filter_length: 10  # length of the convolution filters
initial_state_attention: False

encoders:
  - name: fr
    vocab_size: 9218

decoder:
    name: en
    vocab_size: 7186
