# Really small model on BTEC for debugging purposes

dropout_rate: 0.0
steps_per_checkpoint: 50
steps_per_eval: 100
keep_best: 1
batch_size: 16
cell_size: 128
embedding_size: 128
layers: 1
bidir: False
use_lstm: False

data_dir: data/btec
model_dir: models/debug-multi-task
log_file: null

# encoders and decoders with the same names are shared across tasks
# their parameters should be the same (vocab_size, cell_size, etc.)

encoders:
  - name: feats41
    embedding_size: 41
    vocab_size: 0
    cell_size: 128
    layers: 1
    binary: True

decoder:
  name: en
  vocab_size: 7186

tasks:
  - name: task1
    train_prefix: train.1000
    dev_prefix: dev.100
    ratio: 0.5

  - name: task2
    train_prefix: dev.100
    dev_prefix: dev.100
    ratio: 0.5

main_task: task2    # task that matters (checkpoints are sorted according to this task's scores)
