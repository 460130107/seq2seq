# Really small model on BTEC for debugging purposes

dropout_rate: 0.0
steps_per_checkpoint: 50
steps_per_eval: 100
keep_best: 1
batch_size: 16
cell_size: 128
embedding_size: 128
layers: 1
bidir: False
use_lstm: False

data_dir: data/btec_multi
model_dir: models/debug-multi-task
log_file: null

# share_embeddings: False   # TODO: share embeddings between encoder and decoder

# encoders and decoders with the same names are shared across tasks
# their parameters should be the same (vocab_size, cell_size, etc.)
tasks:
  - name: task1
    train_prefix: train
    dev_prefix: dev.100
    vocab_prefix: vocab
    ratio: 0.5
    encoders:
      - name: fr
        shared_name: fr
        vocab_size: 9218
        buckets: [ 5, 10, 20, 50]
    decoder:
        name: en
        vocab_size: 7186
        buckets: [10, 15, 25, 50]

  - name: task2
    train_prefix: train2
    dev_prefix: dev2.100
    vocab_prefix: vocab2
    ratio: 0.5
    encoders:
      - name: fr
        shared_name: fr
        vocab_size: 9218
        buckets: [ 5, 10, 20, 50]
    decoder:
        name: fr  # same name?  FIXME
        shared_name: fr
        vocab_size: 9218
        buckets: [10, 15, 25, 50]

main_task: task1    # task that matters (checkpoints are sorted according to this task's scores)