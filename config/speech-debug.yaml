
learning_rate_decay_factor: 0.99
dropout_rate: 0.5
bidir: True
use_lstm: True

data_dir: data/btec_speech
model_dir: models/speech_debug
# log_file: models/speech_debug/log.txt
log_file: null

max_output_len: 25
max_input_len: 600
parallel_iterations: 32
swap_memory: True
steps_per_checkpoint: 50
steps_per_eval: 200
# batch_size: 16
optimizer: 'adam'
learning_rate: 0.001

train_prefix: train.1000
dev_prefix: dev.100
num_sample: 0

encoders:
  - name: feats
    embedding_size: 123
    vocab_size: 0
    cell_size: 256
    layers: 3
    time_pooling: [2, 2]
    pooling_avg: True
    binary: True
    attention_filters: 1
    attention_filter_length: 20
    input_layers: [256, 256]
    # attention_window_size: 10

decoder:
    name: en
    vocab_size: 7186
    cell_size: 256
    embedding_size: 256
    layers: 2
