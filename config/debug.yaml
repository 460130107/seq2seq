# Really small model on BTEC for debugging purposes

learning_rate_decay_factor: 0.95
dropout_rate: 0.2
cell_size: 128
embedding_size: 128
layers: 1
bidir: False
use_lstm: True
steps_per_checkpoint: 100
steps_per_eval: 200
max_steps: 50000
max_output_len: 20

data_dir: data/btec
model_dir: models/debug
log_file: null
dev_prefix: dev.100

optimizer: 'adam'
learning_rate: 0.001
# buckets: [[5, 10], [10, 15], [20, 25], [50, 50]]

encoders:
  - name: fr
    vocab_size: 9218
    layers: 3
    time_pooling: [2, 2]

decoder:
    name: en
    vocab_size: 7186
