# Really small model on BTEC for debugging purposes

learning_rate_decay_factor: 0.95
dropout_rate: 0.0
cell_size: 128
embedding_size: 128
layers: 1
bidir: True
use_lstm: True
steps_per_checkpoint: 100
steps_per_eval: 200
max_steps: 50000
max_output_len: 20

data_dir: data/btec
model_dir: models/debug
log_file: null
dev_prefix: dev.100

# optimizer: 'adam'
# learning_rate: 0.001
# buckets: [[5, 10], [10, 15], [20, 25], [50, 50]]
# pooling_avg: True
# attention_window_size: 4 # if positive, use a local attention mechanism with this window size
# attention_filters: 1     # number of convolution filters to use in the attention mechanism
# attention_filter_length: 10  # length of the convolution filters

encoders:
  - name: fr
    vocab_size: 9218

decoder:
    name: en
    vocab_size: 7186
