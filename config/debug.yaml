# Really small model on BTEC for debugging purposes

dropout_rate: 0.0
cell_size: 128
embedding_size: 128
layers: 2
bidir: True
use_lstm: True
steps_per_checkpoint: 100
steps_per_eval: 200
max_steps: 20000
max_output_len: 20

data_dir: data/btec
model_dir: models/debug
log_file: null
dev_prefix: dev.100
num_samples: 0

optimizer: 'adam'
learning_rate: 0.001

encoders:
  - name: fr
    vocab_size: 10000

decoder:
    name: en
    ext: en
    vocab_size: 10000
